{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBwSz3M_7W-o"
   },
   "source": [
    "# Basic Hand Gestures Classification Based on Surface Electromyography (SEMG)\n",
    "\n",
    "Utilizaremos o método CRISP-DM para realizar a tarefa de classificação dos dados a seguir:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/sEMG+for+Basic+Hand+movements\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-kfWbIX_RKH"
   },
   "source": [
    "## O que é CRISP-DM?\n",
    "\n",
    "O *Cross Industry Standard Process for Data Mining* (CRISP-DM) é um modelo de processo com seis fases que descreve naturalmente o ciclo de vida da ciência dos dados. É como um conjunto de guias para ajudar a planear, organizar, e implementar o seu projeto de ciência de dados (ou aprendizagem de máquina). Os 6 guias são:\n",
    "\n",
    "    Compreensão do negócio - O que é que o negócio precisa?\n",
    "    Compreensão de dados - Que dados temos / precisamos? Está limpo?\n",
    "    Preparação dos dados - Como organizamos os dados para a modelagem?\n",
    "    Modelação - Que técnicas de modelação devemos aplicar?\n",
    "    Avaliação - Qual o modelo que melhor satisfaz os objectivos do negócio?\n",
    "    Implantação - Como é que as partes interessadas acedem aos resultados?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoMkOdKn--Ap"
   },
   "source": [
    "## Compreensão do negócio e compreensão dos dados\n",
    "\n",
    "O *dataset* inclui 2 bases de dados de sinais eletromiográficos de superfície de 6 movimentos da mão utilizando o Sistema EMG da Delsys. Os sujeitos saudáveis realizaram seis movimentos diários de apreensão. A eletromiografia de superfície (sEMG) é um método não invasivo de medição da atividade bioelétrica dos músculos, ele é utilizado tanto no diagnóstico de doenças do sistema muscular como no desenvolvimento de interfaces homem-máquina. \n",
    "\n",
    "Os participantes realizaram repetidamente os seis movimentos seguintes:\n",
    "\n",
    "> a) Esférico: para segurar ferramentas esféricas;\n",
    "\n",
    "> b) Ponta: para segurar pequenas ferramentas;\n",
    "\n",
    "> c) Palmar: para agarrar com a palma da mão virada para o objeto;\n",
    "\n",
    "> d) Lateral: para segurar objetos finos e planos;\n",
    "\n",
    "> e) Cilíndrico: para segurar ferramentas cilíndricas;\n",
    "\n",
    "> f) Gancho: para suportar uma carga pesada.\n",
    "\n",
    "\n",
    "\n",
    "Estão incluídas duas bases de dados diferentes:\n",
    "\n",
    "* 5 indivíduos saudáveis (dois homens e três mulheres) da mesma idade aproximadamente (20 a 22 anos) conduziram os 6 movimentos 30 vezes cada um. O tempo medido é de 6 segundos. Há um arquivo para cada participante na base de dados.\n",
    "* 1 sujeito saudável (masculino, 22 anos de idade) conduziu os 6 movimentos 100 vezes cada um, totalizando um total de 600 movimentos, durante 3 dias consecutivos. O tempo medido é de 5 segundos. Há um arquivo apenas para ele. \n",
    "\n",
    "O problema apresentado no conjunto é um problema de classificação, onde é necessário identificar a classe (o gesto que está sendo feito) de cada linha. É interessante notar que utilizar o segundo dataset poderá resultar em uma acurácia maior do que o primeiro, pois é possível que um indivíduo possua algum vício na sua execução de algum gesto, assim realizando um mesmo gesto de uma forma muito semelhante todas as vezes que houver repetição dele. Como consequência, devido a uma forte correlação entre os sinais sEMG, uma precisão de classificação excepcional pode ser alcançada, mas um sobreajustamento significativo do classificador pode desfocar as conclusões.\n",
    "\n",
    "Entretanto o primeiro conjunto de dados também possui uma desvantagem, ele carece no número de repetições de gestos, há apenas 30 repetições de cada gesto, as repetições são vitais para o desenvolvimento de algoritmos robustos de reconhecimento. Também é desaconselhado que se realize o mesmo gesto várias vezes seguidas, pois é provável que o sujeito realize o gesto de uma forma muito semelhante, o que também pode ter como consequência uma precisão de classificação muito alta. Abordagens que reduzem o risco de um sujeito executar cada gesto de forma idêntica são executar sequencialmente uma ordem de gestos ou fazer eles em ordem aleatória, no entanto ainda é vital incluir um grande número de repetições para diminuir o risco. \n",
    "\n",
    "\n",
    "Iremos utilizar a seguinte base de dados para o nosso estudo:\n",
    "\n",
    "```\n",
    "* 1 sujeito saudável (masculino, 22 anos de idade) conduziu os 6 movimentos 100 vezes cada um, totalizando um total de 600 movimentos, durante 3 dias consecutivos. O tempo medido é de 5 segundos. Há um arquivo apenas para ele. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZW337mjo77aG"
   },
   "source": [
    "## Compreensão dos dados e preparação dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFGlHX9BARVO"
   },
   "source": [
    "### Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SPeRtkcRtc4I",
    "outputId": "617e51f5-2756-489e-df7c-c2b12afc1a8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for using Pysiology. If you use it in your work, please cite:\n",
      "Gabrieli G., Azhari A., Esposito G. (2020) PySiology: A Python Package for Physiological Feature Extraction. In: Esposito A., Faundez-Zanuy M., Morabito F., Pasero E. (eds) Neural Approaches to Dynamics of Signal Exchanges. Smart Innovation, Systems and Technologies, vol 151. Springer, Singapore\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date, time\n",
    "import pandas as pd\n",
    "#from google.colab import drive\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import sem\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV# Number of trees in random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from matplotlib import pyplot\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "import pysiology.electromyography as electromyography\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#drive.mount('/content/gdrive')\n",
    "np.random.seed(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6fGWvag6cFf"
   },
   "source": [
    "### Importando e formatando os dados\n",
    "\n",
    "Nessa seção iremos importar e formatar os dados, o repositório do UCI os disponibiliza em formato .mat, portanto devemos transformar esses dados para que possam ser utilizados com a biblioteca pandas do python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65Jk8nIU6T_y",
    "outputId": "d912ad90-c054-4acd-ecb8-f4d7d9354b9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__header__': b'MATLAB 5.0 MAT-file, Platform: PCWIN64, Created on: Tue Nov 18 12:44:17 2014', '__version__': '1.0', '__globals__': [], 'cyl_ch1': array([[ 0.123201,  0.199706,  0.276211, ...,  0.123201,  0.174205,\n",
      "         0.403719],\n",
      "       [ 0.123201,  0.225208,  0.199706, ..., -0.233821,  0.021195,\n",
      "         0.0977  ],\n",
      "       [ 0.174205,  0.225208,  0.123201, ...,  0.0977  ,  0.199706,\n",
      "        -0.029808],\n",
      "       ...,\n",
      "       [ 0.0977  ,  0.174205,  0.199706, ...,  0.0977  ,  0.148703,\n",
      "         0.0977  ],\n",
      "       [ 0.225208,  0.148703,  0.123201, ...,  0.148703,  0.148703,\n",
      "         0.0977  ],\n",
      "       [ 0.123201,  0.199706,  0.072198, ...,  0.174205,  0.021195,\n",
      "         0.046696]]), 'cyl_ch2': array([[ 0.278708,  0.380739,  0.380739, ...,  0.227693, -0.154923,\n",
      "         0.125662],\n",
      "       [ 0.431755,  0.125662,  0.508278, ...,  0.559294,  0.533786,\n",
      "         0.992925],\n",
      "       [ 0.584801, -0.0784  ,  0.329724, ..., -0.741602,  0.329724,\n",
      "        -0.486524],\n",
      "       ...,\n",
      "       [ 0.635817,  0.329724,  0.584801, ..., -0.052892,  0.227693,\n",
      "         0.355232],\n",
      "       [ 0.304216,  0.71234 , -0.486524, ...,  0.508278, -0.001877,\n",
      "        -0.205939],\n",
      "       [-0.818125, -0.410001,  0.227693, ..., -0.0784  , -0.129416,\n",
      "        -0.001877]]), 'hook_ch1': array([[ 0.0977  ,  0.148703,  0.199706, ...,  0.225208, -0.05531 ,\n",
      "        -0.106313],\n",
      "       [ 0.123201,  0.123201,  0.0977  , ...,  0.276211,  0.199706,\n",
      "         0.225208],\n",
      "       [ 0.072198,  0.072198,  0.0977  , ...,  0.174205,  0.225208,\n",
      "         0.199706],\n",
      "       ...,\n",
      "       [ 0.225208,  0.225208,  0.199706, ...,  0.123201,  0.199706,\n",
      "         0.046696],\n",
      "       [ 0.148703,  0.174205,  0.225208, ...,  0.199706,  0.123201,\n",
      "         0.0977  ],\n",
      "       [ 0.225208,  0.123201,  0.123201, ...,  0.199706,  0.276211,\n",
      "         0.276211]]), 'hook_ch2': array([[ 0.253201,  0.15117 ,  0.176677, ..., -0.690586,  0.94191 ,\n",
      "         0.763355],\n",
      "       [ 0.202185,  0.176677,  0.125662, ...,  0.202185,  0.788863,\n",
      "         0.380739],\n",
      "       [ 0.227693,  0.202185,  0.253201, ...,  0.661324,  0.355232,\n",
      "        -0.86914 ],\n",
      "       ...,\n",
      "       [ 0.253201,  0.100154,  0.100154, ...,  0.074646,  0.074646,\n",
      "         0.278708],\n",
      "       [ 0.227693,  0.100154,  0.125662, ...,  0.278708,  0.049139,\n",
      "         0.253201],\n",
      "       [ 0.304216,  0.15117 ,  0.125662, ...,  0.15117 ,  0.100154,\n",
      "        -0.001877]]), 'tip_ch1': array([[ 0.148703,  0.123201,  0.174205, ...,  0.225208,  0.148703,\n",
      "         0.174205],\n",
      "       [ 0.25071 ,  0.199706,  0.174205, ...,  0.123201,  0.123201,\n",
      "        -0.131815],\n",
      "       [ 0.0977  ,  0.225208,  0.199706, ...,  0.123201,  0.123201,\n",
      "         0.123201],\n",
      "       ...,\n",
      "       [ 0.199706,  0.199706,  0.199706, ...,  0.174205,  0.0977  ,\n",
      "         0.0977  ],\n",
      "       [ 0.0977  ,  0.0977  ,  0.072198, ...,  0.148703,  0.0977  ,\n",
      "         0.225208],\n",
      "       [ 0.174205,  0.123201,  0.148703, ...,  0.072198,  0.148703,\n",
      "         0.25071 ]]), 'tip_ch2': array([[ 0.227693,  0.202185, -0.129416, ...,  0.559294,  0.508278,\n",
      "        -0.027385],\n",
      "       [ 0.227693,  0.023631,  0.176677, ...,  0.202185,  0.763355,\n",
      "         0.48277 ],\n",
      "       [ 0.635817,  0.406247,  0.278708, ...,  0.355232,  0.457263,\n",
      "        -0.512032],\n",
      "       ...,\n",
      "       [ 0.202185,  0.15117 ,  0.15117 , ..., -0.384493, -2.731206,\n",
      "         0.304216],\n",
      "       [ 0.202185,  0.202185,  0.049139, ..., -1.251757,  0.890894,\n",
      "         0.023631],\n",
      "       [ 0.406247,  0.074646,  0.15117 , ..., -0.231447,  0.406247,\n",
      "         0.48277 ]]), 'palm_ch1': array([[ 0.0977  ,  0.072198,  0.072198, ..., -0.004307,  0.148703,\n",
      "         0.123201],\n",
      "       [ 0.0977  ,  0.123201,  0.123201, ...,  0.174205,  0.199706,\n",
      "         0.123201],\n",
      "       [ 0.072198,  0.072198,  0.123201, ..., -0.641848,  0.352716,\n",
      "         0.505726],\n",
      "       ...,\n",
      "       [ 0.123201,  0.199706,  0.148703, ...,  0.148703,  0.123201,\n",
      "         0.199706],\n",
      "       [ 0.174205,  0.123201,  0.072198, ...,  0.148703,  0.123201,\n",
      "         0.174205],\n",
      "       [ 0.199706,  0.174205,  0.174205, ...,  0.199706,  0.225208,\n",
      "         0.174205]]), 'palm_ch2': array([[ 0.406247,  0.227693,  0.15117 , ...,  0.176677, -0.027385,\n",
      "        -0.435509],\n",
      "       [ 0.202185,  0.202185,  0.15117 , ..., -0.512032,  0.125662,\n",
      "         0.227693],\n",
      "       [ 0.227693,  0.202185,  0.15117 , ..., -0.052892, -0.103908,\n",
      "         0.049139],\n",
      "       ...,\n",
      "       [ 0.278708,  0.176677, -0.001877, ...,  0.227693, -0.30797 ,\n",
      "         0.610309],\n",
      "       [ 0.355232,  0.253201,  0.355232, ..., -0.256954, -0.052892,\n",
      "         0.125662],\n",
      "       [ 0.355232,  0.304216,  0.15117 , ...,  0.278708,  0.355232,\n",
      "        -0.256954]]), 'spher_ch1': array([[ 0.276211,  0.123201,  0.174205, ...,  0.174205,  0.199706,\n",
      "         0.174205],\n",
      "       [ 0.072198,  0.046696,  0.072198, ..., -0.310326,  0.403719,\n",
      "         0.505726],\n",
      "       [ 0.174205,  0.174205,  0.148703, ...,  0.174205,  0.174205,\n",
      "         0.148703],\n",
      "       ...,\n",
      "       [ 0.123201,  0.174205,  0.174205, ...,  0.148703, -0.106313,\n",
      "        -0.412333],\n",
      "       [ 0.25071 ,  0.225208,  0.199706, ..., -0.080812,  0.123201,\n",
      "         0.072198],\n",
      "       [ 0.199706,  0.25071 ,  0.123201, ..., -0.080812,  0.454723,\n",
      "         0.862749]]), 'spher_ch2': array([[ 0.253201,  0.253201,  0.074646, ..., -0.843633, -0.971171,\n",
      "        -1.506834],\n",
      "       [ 0.227693,  0.227693,  0.023631, ..., -0.0784  ,  0.355232,\n",
      "        -0.103908],\n",
      "       [ 0.227693,  0.355232,  0.329724, ...,  0.992925,  0.890894,\n",
      "         0.227693],\n",
      "       ...,\n",
      "       [-0.256954,  0.176677,  0.355232, ...,  0.763355, -0.027385,\n",
      "         0.661324],\n",
      "       [ 0.202185,  0.457263,  0.304216, ...,  0.788863,  0.48277 ,\n",
      "        -0.384493],\n",
      "       [ 0.457263,  0.304216, -0.512032, ..., -0.588555,  1.222495,\n",
      "         0.559294]]), 'lat_ch1': array([[ 0.174205,  0.123201,  0.199706, ..., -0.004307,  0.276211,\n",
      "         0.123201],\n",
      "       [ 0.123201,  0.199706,  0.225208, ...,  0.225208,  0.199706,\n",
      "         0.199706],\n",
      "       [ 0.225208,  0.148703,  0.174205, ...,  0.174205, -0.004307,\n",
      "        -0.004307],\n",
      "       ...,\n",
      "       [ 0.123201,  0.148703,  0.123201, ...,  0.480224,  0.327214,\n",
      "         0.072198],\n",
      "       [ 0.148703,  0.148703,  0.148703, ...,  0.072198, -0.106313,\n",
      "         0.276211],\n",
      "       [ 0.148703,  0.199706,  0.199706, ...,  0.225208,  0.225208,\n",
      "         0.072198]]), 'lat_ch2': array([[ 0.227693,  0.202185,  0.253201, ...,  0.814371,  0.278708,\n",
      "         0.584801],\n",
      "       [ 0.227693,  0.176677,  0.253201, ...,  0.48277 , -0.588555,\n",
      "         0.661324],\n",
      "       [ 0.380739,  0.100154, -0.027385, ...,  0.380739, -0.945664,\n",
      "        -0.001877],\n",
      "       ...,\n",
      "       [ 0.457263, -0.639571, -0.103908, ...,  0.533786, -0.614063,\n",
      "        -1.32828 ],\n",
      "       [ 0.202185,  0.202185,  0.227693, ..., -0.052892, -0.129416,\n",
      "         0.559294],\n",
      "       [ 0.227693,  0.278708,  0.202185, ...,  0.304216,  0.304216,\n",
      "        -0.30797 ]])}\n"
     ]
    }
   ],
   "source": [
    "#matfile = '/content/gdrive/My Drive/data/female_1.mat'\n",
    "matfile = 'data/male_day_1.mat'\n",
    "mat = loadmat(matfile)\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcgNUrJj8mZl"
   },
   "source": [
    "### Juntando todos os dados dos 3 dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__header__\n",
      "__version__\n",
      "__globals__\n",
      "cyl_ch1\n",
      "cyl_ch2\n",
      "hook_ch1\n",
      "hook_ch2\n",
      "tip_ch1\n",
      "tip_ch2\n",
      "palm_ch1\n",
      "palm_ch2\n",
      "spher_ch1\n",
      "spher_ch2\n",
      "lat_ch1\n",
      "lat_ch2\n"
     ]
    }
   ],
   "source": [
    "matfile = 'data/male_day_1.mat'\n",
    "mat = loadmat(matfile)\n",
    "for key,values in mat.items():\n",
    "    print(key)\n",
    "    \n",
    "df = pd.DataFrame(mat['hook_ch1'])\n",
    "df[\"hand_mov\"] = \"hook\"\n",
    "\n",
    "df1 = pd.DataFrame(mat['cyl_ch1'])\n",
    "df1[\"hand_mov\"] = \"cyl\"\n",
    "\n",
    "df2 = pd.DataFrame(mat['tip_ch1'])\n",
    "df2[\"hand_mov\"] = \"tip\"\n",
    "\n",
    "df3 = pd.DataFrame(mat['spher_ch1'])\n",
    "df3[\"hand_mov\"] = \"spher\"\n",
    "\n",
    "df4 = pd.DataFrame(mat['palm_ch1'])\n",
    "df4[\"hand_mov\"] = \"palm\"\n",
    "\n",
    "df5 = pd.DataFrame(mat['lat_ch1'])\n",
    "df5[\"hand_mov\"] = \"lat\"\n",
    "\n",
    "df_all = pd.concat([df,df1,df2,df3,df4,df5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__header__\n",
      "__version__\n",
      "__globals__\n",
      "cyl_ch1\n",
      "cyl_ch2\n",
      "hook_ch1\n",
      "hook_ch2\n",
      "tip_ch1\n",
      "tip_ch2\n",
      "palm_ch1\n",
      "palm_ch2\n",
      "spher_ch1\n",
      "spher_ch2\n",
      "lat_ch1\n",
      "lat_ch2\n"
     ]
    }
   ],
   "source": [
    "matfile = 'data/male_day_2.mat'\n",
    "mat = loadmat(matfile)\n",
    "for key,values in mat.items():\n",
    "    print(key)\n",
    "    \n",
    "df = pd.DataFrame(mat['hook_ch1'])\n",
    "df[\"hand_mov\"] = \"hook\"\n",
    "\n",
    "df1 = pd.DataFrame(mat['cyl_ch1'])\n",
    "df1[\"hand_mov\"] = \"cyl\"\n",
    "\n",
    "df2 = pd.DataFrame(mat['tip_ch1'])\n",
    "df2[\"hand_mov\"] = \"tip\"\n",
    "\n",
    "df3 = pd.DataFrame(mat['spher_ch1'])\n",
    "df3[\"hand_mov\"] = \"spher\"\n",
    "\n",
    "df4 = pd.DataFrame(mat['palm_ch1'])\n",
    "df4[\"hand_mov\"] = \"palm\"\n",
    "\n",
    "df5 = pd.DataFrame(mat['lat_ch1'])\n",
    "df5[\"hand_mov\"] = \"lat\"\n",
    "\n",
    "df_all1 = pd.concat([df,df1,df2,df3,df4,df5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__header__\n",
      "__version__\n",
      "__globals__\n",
      "cyl_ch1\n",
      "cyl_ch2\n",
      "hook_ch1\n",
      "hook_ch2\n",
      "tip_ch1\n",
      "tip_ch2\n",
      "palm_ch1\n",
      "palm_ch2\n",
      "spher_ch1\n",
      "spher_ch2\n",
      "lat_ch1\n",
      "lat_ch2\n"
     ]
    }
   ],
   "source": [
    "matfile = 'data/male_day_3.mat'\n",
    "mat = loadmat(matfile)\n",
    "for key,values in mat.items():\n",
    "    print(key)\n",
    "    \n",
    "df = pd.DataFrame(mat['hook_ch1'])\n",
    "df[\"hand_mov\"] = \"hook\"\n",
    "\n",
    "df1 = pd.DataFrame(mat['cyl_ch1'])\n",
    "df1[\"hand_mov\"] = \"cyl\"\n",
    "\n",
    "df2 = pd.DataFrame(mat['tip_ch1'])\n",
    "df2[\"hand_mov\"] = \"tip\"\n",
    "\n",
    "df3 = pd.DataFrame(mat['spher_ch1'])\n",
    "df3[\"hand_mov\"] = \"spher\"\n",
    "\n",
    "df4 = pd.DataFrame(mat['palm_ch1'])\n",
    "df4[\"hand_mov\"] = \"palm\"\n",
    "\n",
    "df5 = pd.DataFrame(mat['lat_ch1'])\n",
    "df5[\"hand_mov\"] = \"lat\"\n",
    "\n",
    "df_all2 = pd.concat([df,df1,df2,df3,df4,df5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_all,df_all1,df_all2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIBi_AUVPdAP"
   },
   "source": [
    "## Analise de estatística descritiva dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GdN3HjeLPSF-",
    "outputId": "3590c83b-4f96-4d01-eafa-ff2a8dce351b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 2501)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "UK6QYk4eSu11",
    "outputId": "a1d2ac53-7099-443f-a56b-2a79dc5f9975"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2490</th>\n",
       "      <th>2491</th>\n",
       "      <th>2492</th>\n",
       "      <th>2493</th>\n",
       "      <th>2494</th>\n",
       "      <th>2495</th>\n",
       "      <th>2496</th>\n",
       "      <th>2497</th>\n",
       "      <th>2498</th>\n",
       "      <th>2499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.150800</td>\n",
       "      <td>0.149284</td>\n",
       "      <td>0.147499</td>\n",
       "      <td>0.144212</td>\n",
       "      <td>0.143135</td>\n",
       "      <td>0.147499</td>\n",
       "      <td>0.150927</td>\n",
       "      <td>0.149468</td>\n",
       "      <td>0.146578</td>\n",
       "      <td>0.151551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158195</td>\n",
       "      <td>0.157359</td>\n",
       "      <td>0.157430</td>\n",
       "      <td>0.145827</td>\n",
       "      <td>0.138361</td>\n",
       "      <td>0.139650</td>\n",
       "      <td>0.153336</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.145614</td>\n",
       "      <td>0.140160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.086425</td>\n",
       "      <td>0.100784</td>\n",
       "      <td>0.095723</td>\n",
       "      <td>0.093908</td>\n",
       "      <td>0.106889</td>\n",
       "      <td>0.093053</td>\n",
       "      <td>0.123015</td>\n",
       "      <td>0.094398</td>\n",
       "      <td>0.096322</td>\n",
       "      <td>0.104195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254935</td>\n",
       "      <td>0.274816</td>\n",
       "      <td>0.265947</td>\n",
       "      <td>0.269613</td>\n",
       "      <td>0.305276</td>\n",
       "      <td>0.297503</td>\n",
       "      <td>0.297020</td>\n",
       "      <td>0.277375</td>\n",
       "      <td>0.323763</td>\n",
       "      <td>0.297136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.539841</td>\n",
       "      <td>-1.100877</td>\n",
       "      <td>-1.228385</td>\n",
       "      <td>-0.794857</td>\n",
       "      <td>-1.789421</td>\n",
       "      <td>-0.616346</td>\n",
       "      <td>-2.069939</td>\n",
       "      <td>-0.922366</td>\n",
       "      <td>-0.896864</td>\n",
       "      <td>-1.381395</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.891428</td>\n",
       "      <td>-2.962496</td>\n",
       "      <td>-2.069939</td>\n",
       "      <td>-3.243014</td>\n",
       "      <td>-2.962496</td>\n",
       "      <td>-3.447027</td>\n",
       "      <td>-3.039001</td>\n",
       "      <td>-2.528968</td>\n",
       "      <td>-6.915249</td>\n",
       "      <td>-4.186575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.123201</td>\n",
       "      <td>0.097700</td>\n",
       "      <td>0.097700</td>\n",
       "      <td>0.097700</td>\n",
       "      <td>0.097700</td>\n",
       "      <td>0.097700</td>\n",
       "      <td>0.097700</td>\n",
       "      <td>0.123201</td>\n",
       "      <td>0.097700</td>\n",
       "      <td>0.097700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072198</td>\n",
       "      <td>0.072198</td>\n",
       "      <td>0.097700</td>\n",
       "      <td>0.072198</td>\n",
       "      <td>0.072198</td>\n",
       "      <td>0.072198</td>\n",
       "      <td>0.072198</td>\n",
       "      <td>0.072198</td>\n",
       "      <td>0.072198</td>\n",
       "      <td>0.072198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.148703</td>\n",
       "      <td>0.148703</td>\n",
       "      <td>0.148703</td>\n",
       "      <td>0.148703</td>\n",
       "      <td>0.148703</td>\n",
       "      <td>0.148703</td>\n",
       "      <td>0.148703</td>\n",
       "      <td>0.148703</td>\n",
       "      <td>0.148703</td>\n",
       "      <td>0.148703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148703</td>\n",
       "      <td>0.148703</td>\n",
       "      <td>0.148703</td>\n",
       "      <td>0.148703</td>\n",
       "      <td>0.148703</td>\n",
       "      <td>0.148703</td>\n",
       "      <td>0.148703</td>\n",
       "      <td>0.148703</td>\n",
       "      <td>0.148703</td>\n",
       "      <td>0.148703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.199706</td>\n",
       "      <td>0.199706</td>\n",
       "      <td>0.199706</td>\n",
       "      <td>0.199706</td>\n",
       "      <td>0.199706</td>\n",
       "      <td>0.199706</td>\n",
       "      <td>0.199706</td>\n",
       "      <td>0.199706</td>\n",
       "      <td>0.199706</td>\n",
       "      <td>0.199706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225208</td>\n",
       "      <td>0.225208</td>\n",
       "      <td>0.225208</td>\n",
       "      <td>0.225208</td>\n",
       "      <td>0.225208</td>\n",
       "      <td>0.225208</td>\n",
       "      <td>0.225208</td>\n",
       "      <td>0.225208</td>\n",
       "      <td>0.225208</td>\n",
       "      <td>0.225208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.372781</td>\n",
       "      <td>1.959319</td>\n",
       "      <td>0.837247</td>\n",
       "      <td>1.219772</td>\n",
       "      <td>1.015759</td>\n",
       "      <td>1.092263</td>\n",
       "      <td>2.188834</td>\n",
       "      <td>1.219772</td>\n",
       "      <td>1.092263</td>\n",
       "      <td>1.347280</td>\n",
       "      <td>...</td>\n",
       "      <td>2.800873</td>\n",
       "      <td>2.367345</td>\n",
       "      <td>2.724368</td>\n",
       "      <td>1.933817</td>\n",
       "      <td>2.316342</td>\n",
       "      <td>1.857312</td>\n",
       "      <td>3.387410</td>\n",
       "      <td>1.908316</td>\n",
       "      <td>2.341844</td>\n",
       "      <td>2.137830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 2500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0            1            2            3            4     \\\n",
       "count  1800.000000  1800.000000  1800.000000  1800.000000  1800.000000   \n",
       "mean      0.150800     0.149284     0.147499     0.144212     0.143135   \n",
       "std       0.086425     0.100784     0.095723     0.093908     0.106889   \n",
       "min      -0.539841    -1.100877    -1.228385    -0.794857    -1.789421   \n",
       "25%       0.123201     0.097700     0.097700     0.097700     0.097700   \n",
       "50%       0.148703     0.148703     0.148703     0.148703     0.148703   \n",
       "75%       0.199706     0.199706     0.199706     0.199706     0.199706   \n",
       "max       1.372781     1.959319     0.837247     1.219772     1.015759   \n",
       "\n",
       "              5            6            7            8            9     ...  \\\n",
       "count  1800.000000  1800.000000  1800.000000  1800.000000  1800.000000  ...   \n",
       "mean      0.147499     0.150927     0.149468     0.146578     0.151551  ...   \n",
       "std       0.093053     0.123015     0.094398     0.096322     0.104195  ...   \n",
       "min      -0.616346    -2.069939    -0.922366    -0.896864    -1.381395  ...   \n",
       "25%       0.097700     0.097700     0.123201     0.097700     0.097700  ...   \n",
       "50%       0.148703     0.148703     0.148703     0.148703     0.148703  ...   \n",
       "75%       0.199706     0.199706     0.199706     0.199706     0.199706  ...   \n",
       "max       1.092263     2.188834     1.219772     1.092263     1.347280  ...   \n",
       "\n",
       "              2490         2491         2492         2493         2494  \\\n",
       "count  1800.000000  1800.000000  1800.000000  1800.000000  1800.000000   \n",
       "mean      0.158195     0.157359     0.157430     0.145827     0.138361   \n",
       "std       0.254935     0.274816     0.265947     0.269613     0.305276   \n",
       "min      -1.891428    -2.962496    -2.069939    -3.243014    -2.962496   \n",
       "25%       0.072198     0.072198     0.097700     0.072198     0.072198   \n",
       "50%       0.148703     0.148703     0.148703     0.148703     0.148703   \n",
       "75%       0.225208     0.225208     0.225208     0.225208     0.225208   \n",
       "max       2.800873     2.367345     2.724368     1.933817     2.316342   \n",
       "\n",
       "              2495         2496         2497         2498         2499  \n",
       "count  1800.000000  1800.000000  1800.000000  1800.000000  1800.000000  \n",
       "mean      0.139650     0.153336     0.142299     0.145614     0.140160  \n",
       "std       0.297503     0.297020     0.277375     0.323763     0.297136  \n",
       "min      -3.447027    -3.039001    -2.528968    -6.915249    -4.186575  \n",
       "25%       0.072198     0.072198     0.072198     0.072198     0.072198  \n",
       "50%       0.148703     0.148703     0.148703     0.148703     0.148703  \n",
       "75%       0.225208     0.225208     0.225208     0.225208     0.225208  \n",
       "max       1.857312     3.387410     1.908316     2.341844     2.137830  \n",
       "\n",
       "[8 rows x 2500 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQw34L5VZEwM",
    "outputId": "bdc518e6-a1b3-45c4-cb03-c6382ee0bf68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           float64\n",
       "1           float64\n",
       "2           float64\n",
       "3           float64\n",
       "4           float64\n",
       "             ...   \n",
       "2496        float64\n",
       "2497        float64\n",
       "2498        float64\n",
       "2499        float64\n",
       "hand_mov     object\n",
       "Length: 2501, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BfHZa7CPRs2"
   },
   "source": [
    "### Dados Nulos\n",
    "\n",
    "Sempre é importante verificar se existem dados nulos em um conjunto de dados, para que seja feito seu tratamento ou sua retirada, caso seja necessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 552
    },
    "id": "v6bTXmYVQUks",
    "outputId": "1cd3eb46-8dd0-447b-e0de-89fdf6f632f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8m9xr0VmV2rl"
   },
   "source": [
    "No entanto aqui observamos que não há dados nulos nesse conjunto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEcgOZ04QQcT"
   },
   "source": [
    "### Distribuição das classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D2H6iYFOPgVK",
    "outputId": "feb983f4-77d0-4f4c-c868-c5ac6ed56114"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hand_mov\n",
       "cyl      300\n",
       "hook     300\n",
       "lat      300\n",
       "palm     300\n",
       "spher    300\n",
       "tip      300\n",
       "Name: hand_mov, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribuição das classes\n",
    "df_all[['hand_mov']].groupby('hand_mov')['hand_mov'].count() # Contando a quantidade de classes e suas respectivas quantidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "DvbOiYAxPxJI",
    "outputId": "e4f67715-4e76-4f57-cdcc-b821809795b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.hist(df_all[\"hand_mov\"],bins=12)\n",
    "plt.gca()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQW6ONCeRDf0"
   },
   "source": [
    "Temos 6 classes nesse dataset, sendo elas nomeadas como: cyl, hook, tip, palm, spher e lat. Podemos observar que as classes são perfeitamente balanceadas, portanto não há necessidade de qualquer técnica de subsampling ou resampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "in8692YQRVhH"
   },
   "source": [
    "### Matriz de correlação\n",
    "\n",
    "A correlação é uma estatística muito comum e muito utilizada para analisar se há ligações entre variáveis ou se algum parâmetro pode ser um bom preditor para outro, portanto, tendo em mente que não se pode concluir que correlação implica em causualidade, é interessante plotar a matriz de correlação das variáveis do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Absolute Correlations\n",
      "453  483    0.736354\n",
      "473  483    0.719204\n",
      "398  401    0.718001\n",
      "456  483    0.690818\n",
      "462  465    0.686015\n",
      "441  483    0.662099\n",
      "483  485    0.656877\n",
      "514  518    0.650912\n",
      "472  473    0.650725\n",
      "555  556    0.634434\n",
      "462  483    0.633687\n",
      "554  556    0.633059\n",
      "398  402    0.632042\n",
      "453  456    0.623015\n",
      "514  517    0.621993\n",
      "482  483    0.619750\n",
      "441  473    0.618563\n",
      "472  475    0.615063\n",
      "396  399    0.612066\n",
      "     400    0.611413\n",
      "514  515    0.608354\n",
      "453  482    0.607638\n",
      "483  515    0.602879\n",
      "453  473    0.601943\n",
      "468  483    0.594190\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df, n=5):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]\n",
    "\n",
    "print(\"Top Absolute Correlations\")\n",
    "print(get_top_abs_correlations(df_all.drop(\"hand_mov\",axis=1), 25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não é possível calcular a correlação quando temos uma variável categórica com mais de 2 classes, no entanto podemos realizar um procedimento chamado one-hot-encoding e avaliar a correlação entre as variáveis contínuas e as variáveis criadas com a transformação da categórica. \n",
    "\n",
    "Isto é, iremos avaliar se ch1 e ch2 tem correlação com o fato de uma linha ser classificada como cyl ou não, ou se possuem correlação com a classificação de hook ou não, por exemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cRWRSPOfBfX"
   },
   "source": [
    "One-hot Encoding é um tipo de representação vetorial em que todos os elementos de um vetor são 0, exceto um, que tem como valor 1, onde 1 representa um booleano especificando uma categoria do elemento. Por exemplo no conjunto de dados a coluna hand_mov_palm só tera 1 onde a classe for palm. Ele representa a existência ou não dessa classificação, essa é uma maneira de representar variáveis categóricas como números sem denotar uma ordem de importância entre elas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6  \\\n",
      "0  0.097700  0.148703  0.199706  0.225208  0.250710  0.199706  0.072198   \n",
      "1  0.123201  0.123201  0.097700  0.174205  0.148703  0.021195  0.072198   \n",
      "2  0.072198  0.072198  0.097700  0.174205  0.174205  0.148703  0.148703   \n",
      "3  0.174205  0.174205  0.199706  0.199706  0.174205  0.225208  0.123201   \n",
      "4  0.123201  0.199706  0.097700  0.097700  0.174205  0.174205  0.148703   \n",
      "\n",
      "          7         8         9  ...      2496      2497      2498      2499  \\\n",
      "0  0.046696  0.072198  0.123201  ...  0.786244  0.225208 -0.055310 -0.106313   \n",
      "1  0.199706  0.123201  0.123201  ...  0.225208  0.276211  0.199706  0.225208   \n",
      "2  0.123201  0.123201  0.148703  ...  0.097700  0.174205  0.225208  0.199706   \n",
      "3  0.123201  0.097700  0.072198  ...  0.174205  0.097700  0.123201  0.148703   \n",
      "4  0.123201  0.097700  0.072198  ...  0.072198  0.021195  0.046696  0.021195   \n",
      "\n",
      "   hand_mov_cyl  hand_mov_hook  hand_mov_lat  hand_mov_palm  hand_mov_spher  \\\n",
      "0             0              1             0              0               0   \n",
      "1             0              1             0              0               0   \n",
      "2             0              1             0              0               0   \n",
      "3             0              1             0              0               0   \n",
      "4             0              1             0              0               0   \n",
      "\n",
      "   hand_mov_tip  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n",
      "\n",
      "[5 rows x 2506 columns]\n"
     ]
    }
   ],
   "source": [
    "df_all_ohe = pd.get_dummies(df_all, prefix='hand_mov')\n",
    "print(df_all_ohe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "sJXaTHL0eRV8",
    "outputId": "7c1c38c4-7c51-4cac-beee-50155c2fc411"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Absolute Correlations\n",
      "453   483     0.736354\n",
      "473   483     0.719204\n",
      "398   401     0.718001\n",
      "456   483     0.690818\n",
      "462   465     0.686015\n",
      "441   483     0.662099\n",
      "483   485     0.656877\n",
      "514   518     0.650912\n",
      "472   473     0.650725\n",
      "555   556     0.634434\n",
      "462   483     0.633687\n",
      "554   556     0.633059\n",
      "398   402     0.632042\n",
      "453   456     0.623015\n",
      "514   517     0.621993\n",
      "482   483     0.619750\n",
      "441   473     0.618563\n",
      "472   475     0.615063\n",
      "396   399     0.612066\n",
      "      400     0.611413\n",
      "514   515     0.608354\n",
      "453   482     0.607638\n",
      "483   515     0.602879\n",
      "453   473     0.601943\n",
      "468   483     0.594190\n",
      "515   518     0.592699\n",
      "398   408     0.585931\n",
      "402   408     0.585626\n",
      "483   489     0.584477\n",
      "517   518     0.579705\n",
      "483   484     0.577799\n",
      "402   407     0.570939\n",
      "607   608     0.566343\n",
      "468   470     0.564642\n",
      "485   486     0.560968\n",
      "456   485     0.559868\n",
      "375   377     0.558959\n",
      "483   518     0.558788\n",
      "458   470     0.556614\n",
      "473   482     0.555768\n",
      "595   596     0.555166\n",
      "441   472     0.554775\n",
      "1664  1665    0.553471\n",
      "2069  2070    0.552257\n",
      "456   473     0.550351\n",
      "472   483     0.547505\n",
      "462   473     0.547417\n",
      "582   583     0.546643\n",
      "473   491     0.546191\n",
      "441   453     0.546080\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Absolute Correlations\")\n",
    "print(get_top_abs_correlations(df_all_ohe, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNmmBvarfz1Q"
   },
   "source": [
    "Podemos concluir nossa observação, não há correlação relevante entre os preditores e as classes. Apenas entre os preditores entre si, o que deve ser tratado, preferencialmente com algum tipo de feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2sWVMTjj6_3"
   },
   "source": [
    "### Tratamento de outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQqAEOMRW7Ro"
   },
   "source": [
    "Dado que em nosso conjunto de dados pode haver outliers ou ruídos seria interessante realizar um tratamento para tais dados indesejados.\n",
    "\n",
    "### IsolationForest\n",
    "\n",
    "É um algoritmo de detecção de anomalias baseado em árvores, baseia-se na modelação dos dados normais de modo a isolar anomalias que são simultaneamente poucas em número e diferentes no espaço de características.\n",
    "\n",
    "Talvez o hiperparâmetro mais importante no modelo seja o argumento da \"contaminação\", que é utilizado para ajudar a estimar o número de outliers no conjunto de dados. Este é um valor entre 0,0 e 0,5 e, por padrão, é fixado em 0,1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14-tnQtbXnQ9",
    "outputId": "e1562537-2263-4ccd-b00f-e86ecbad67cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do X_train antes de remover os outliers: (1206, 2500)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def create_X_y_df(df):\n",
    "  # retrieve the array\n",
    "  data = df.values\n",
    "  # split into input and output elements\n",
    "  X, y = data[:, :-1], data[:, -1]\n",
    "  # split into train and test sets\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "  # summarize the shape of the train and test sets\n",
    "  return X_train, X_test, y_train, y_test \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = create_X_y_df(df_all)\n",
    "print(\"Tamanho do X_train antes de remover os outliers: {}\".format(X_train.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNrKOHuYCaDu",
    "outputId": "8bdceb7e-e592-4c8f-c68f-53c1b9a2de90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do X_train após remover os outliers: (1145, 2500)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "# identify outliers in the training dataset\n",
    "iso = IsolationForest(contamination=0.05)\n",
    "yhat = iso.fit_predict(X_train)\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "\n",
    "print(\"Tamanho do X_train após remover os outliers: {}\".format(X_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AI-eaWhyfQiX"
   },
   "source": [
    "Após aplicar o método de detecção de outliers é possível verificar uma diminuição na quantidade de linhas no X_train, as linhas retiradas foram identificadas como outliers e removidas. Apenas poderemos confirmar se a retirada dos outliers melhora o desempenho do modelo ao realizar testes com um modelo baseline e um modelo que possui detecção de outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de avaliação\n",
    "\n",
    "\n",
    "### Recall\n",
    "\n",
    "Segundo o Google Developers para Machine Learning [https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall] , entendemos que o recall responde a seguinte pergunta: qual proporção de positivos foi identificados corretamente? Em outras palavras, quão bom meu modelo é para prever positivos. Ele é definido como a razão entre verdadeiros positivos sobre a soma de verdadeiros positivos com negativos falsos.\n",
    "\n",
    "### Precision\n",
    "\n",
    "Ainda usando o material do Google Developers, precisão é definida como a resposta para a seguinte pergunta: Qual a proporção de identificações positivas foi realmente correta? É a razão entre os verdadeiros positivos sobre verdadeiros positivos + falsos positivos.\n",
    "\n",
    "### Matriz de Confusão\n",
    "\n",
    "A matriz de confusão é tabela que mostra as frequências de classificação para cada classe do modelo. \n",
    "\n",
    "- Verdadeiro positivo (true positive — TP): ocorre quando no conjunto real, a classe que estamos buscando foi prevista corretamente. \n",
    "- Falso positivo (false positive — FP): ocorre quando no conjunto real, a classe que estamos buscando prever foi prevista incorretamente.\n",
    "- Verdadeiro negativo (true negative — TN): ocorre quando no conjunto real, a classe que não estamos buscando prever foi prevista corretamente. \n",
    "- Falso negativo (false negative — FN): ocorre quando no conjunto real, a classe que não estamos buscando prever foi prevista incorretamente. \n",
    "\n",
    "### Accuracy\n",
    "\n",
    "A acurácia é uma métrica para avaliar os modelos de classificação. A acurácia (accuracy score) é a fração das previsões que nosso modelo acertou:\n",
    "\n",
    "Número de predições corretas / número de predições em total\n",
    "\n",
    "Para a classificação binária, a precisão também pode ser calculada em termos de positivos e negativos da seguinte forma: \n",
    "\n",
    "(TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "A acurácia por si só não conta a história completa quando se trabalha com um conjunto de dados desequilibrado, onde há uma disparidade significativa entre o número de etiquetas positivas e negativas. No entanto como nosso conjunto de dados é relativamente equilibrado é seguro utilizar a acurácia em conjunto com outros métodos.\n",
    "\n",
    "\n",
    "### f1 score\n",
    "\n",
    "O f-measure ou f1 score é a média harmônica do precision e recall. Na maioria das situações, você terá um trade-off entre precision e recall. Se seu classificador for otimizado para aumentar um e desfavorecer o outro, a média harmônica diminui rapidamente, no entanto, é maior quando tanto o precision quanto o recall tem valores parecidos.\n",
    "\n",
    "F-measure tem um significado intuitivo. Ela lhe diz quão preciso é seu classificador (quantas instâncias ele classifica corretamente), bem como quão robusto ele é (ele não perde um número significativo de instâncias).\n",
    "\n",
    "Com alta precisão, mas baixo recall, seu classificador é extremamente preciso, mas perde um número significativo de instâncias que são difíceis de classificar e isso não seria muito útil. Portanto o f-measure será a métrica principal que iremos utilizar para comparar os algoritmos de classificação.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "A seleção de características é um dos conceitos centrais na aprendizagem de máquinas que tem um grande impacto no desempenho de um modelo. Os recursos de dados que são utilizados para treinar modelos de aprendizagem de máquinas têm uma enorme influência no desempenho que você pode ser alcançado.\n",
    "\n",
    "A seleção de características é o processo no qual são selecionadas automática ou manualmente as características que mais contribuem para variável de previsão ou saída na qual há interesse. Ter características irrelevantes nos dados pode diminuir a precisão dos modelos e fazer com que o modelo aprenda com base em características não pertinentes.\n",
    "\n",
    "A seleção de caracteristicas tem os seguintes benefícios:\n",
    "\n",
    "- Reduz o excesso de ajuste: Dados menos redundantes significam menos oportunidades para tomar decisões baseadas em ruído.\n",
    "- Melhora a Precisão: Menos dados enganosos significa que a precisão da modelagem melhora.\n",
    "- Reduz o tempo de treinamento: menos pontos de dados reduzem a complexidade do algoritmo e os algoritmos treinam mais rapidamente.\n",
    "\n",
    "Utilizaremos PCA para realizar uma redução de dimensionalidade e consequentemente feature selection, o PCA procura substituir as variáveis p (mais ou menos correlacionadas) por k<p combinações lineares não correlacionadas (projeções) das variáveis originais.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgQQ8BhGJqkh"
   },
   "source": [
    "## Funções Auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8djKEVXUJtLK"
   },
   "source": [
    "### KFold\n",
    "\n",
    "Para todos os modelos iremos realizar um KFold com 10 folds, onde ao se selecionar os indices de treino e teste serão normalizados os dados e depois aplicado o PCA para o X_train e X_test, isso será feito tanto para os dados sem a retirada de outliers como para o conjunto de dados com presença de outliers para fins comparativos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FmT4vj51JpXL"
   },
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "def KFolds_outlier(df_all, model):\n",
    "  outlier_list = []\n",
    "  normal_list = []\n",
    "  normal_f1_list = []\n",
    "  normal_class_repo_list = []\n",
    "\n",
    "  normal_conf_matrix = []\n",
    "  outlier_conf_matrix = []\n",
    "    \n",
    "  outlier_f1_list = []\n",
    "  outlier_class_repo_list = []\n",
    "\n",
    "  df = shuffle(df_all)\n",
    "  data = df.values\n",
    "  X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "  i = 0\n",
    "  kf = StratifiedKFold(n_splits=10,random_state=11, shuffle=True)\n",
    "  kf.get_n_splits(X,y)\n",
    "\n",
    "  print(kf)\n",
    "\n",
    "  for train_index, test_index in kf.split(X,y):\n",
    "\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    i = i + 1\n",
    "    \n",
    "    pca = PCA(.50)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit on training set only.\n",
    "    scaler.fit(X_train)\n",
    "    # Apply transform to both the training set and the test set.\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Fold número: \", i)\n",
    "    print(\"Tamanho do X_train antes de remover os outliers: {}\".format(X_train.shape))\n",
    "\n",
    "    modelo = model\n",
    "    modelo.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred, labels=[\"cyl\", \"hook\", \"tip\", \"palm\", \"spher\", \"lat\"])\n",
    "    class_repo = classification_report(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred,labels=[\"cyl\", \"hook\", \"tip\", \"palm\", \"spher\", \"lat\"], average='weighted')\n",
    "    \n",
    "    print(\"Acurácia antes retirada de outliers: \", acc)\n",
    "    print(\"F1-Score antes retirada de outliers: \", f1)\n",
    "    normal_list.append(acc)\n",
    "    normal_conf_matrix.append(conf_matrix)\n",
    "    normal_f1_list.append(f1)\n",
    "    normal_class_repo_list.append(class_repo)\n",
    "    \n",
    "    # identify outliers in the training dataset\n",
    "    iso = IsolationForest(contamination=0.05)\n",
    "    yhat = iso.fit_predict(X_train)\n",
    "    # select all rows that are not outliers\n",
    "    mask = yhat != -1\n",
    "    X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "    \n",
    "    print(\"Tamanho do X_train após remover os outliers: {}\".format(X_train.shape))\n",
    "\n",
    "    modelo = model\n",
    "    modelo.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred, labels=[\"cyl\", \"hook\", \"tip\", \"palm\", \"spher\", \"lat\"])\n",
    "    class_repo = classification_report(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred,labels=[\"cyl\", \"hook\", \"tip\", \"palm\", \"spher\", \"lat\"], average='weighted')\n",
    "    \n",
    "    print(\"Acurácia após retirada de outliers: \", acc)\n",
    "    print(\"F1-Score após retirada de outliers: \", f1)\n",
    "    \n",
    "    outlier_list.append(acc)\n",
    "    outlier_conf_matrix.append(conf_matrix)\n",
    "    outlier_f1_list.append(f1)\n",
    "    outlier_class_repo_list.append(class_repo)\n",
    "\n",
    "  return normal_list, outlier_list, normal_conf_matrix, outlier_conf_matrix, normal_f1_list, outlier_f1_list, normal_class_repo_list, outlier_class_repo_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZREZeJyuJ3tS"
   },
   "source": [
    "### GridSearch\n",
    "\n",
    "O GridSearch está presente para encontrar a melhor variação paramétrica para cada modelo, nele estamos utilizando o f1_score como métrica principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "e_GgfgJFJ5QI"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "np.random.seed(11)\n",
    "\n",
    "def grid_search(model, param_grid, n_folds, X, y):\n",
    "  scorer = make_scorer(f1_score, average = 'weighted')\n",
    "  #use gridsearch to test all values for n_neighbors\n",
    "  gscv = GridSearchCV(model, param_grid, cv=n_folds, scoring=scorer, verbose=2)\n",
    "  #fit model to data\n",
    "  gscv.fit(X, y)\n",
    "  return gscv.best_params_, gscv.best_score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LzDWJfUKuLw"
   },
   "source": [
    "### Boxplot\n",
    "\n",
    "Iremos apresentar o boxplot com a variação e a média dos folds para o kfold que será realizado em cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9VDr-ySoKyjr"
   },
   "outputs": [],
   "source": [
    "def boxplot_results(normal_list, outlier_list):\n",
    "  results_kfold = pd.DataFrame()\n",
    "  results_kfold[\"mean\"] = normal_list\n",
    "  results_kfold[\"method\"] = \"with outliers\"\n",
    "\n",
    "  results_kfold1 = pd.DataFrame()\n",
    "  results_kfold1[\"mean\"] = outlier_list\n",
    "  results_kfold1[\"method\"] = \"without outliers\"\n",
    "\n",
    "  results_kfold = pd.concat([results_kfold,results_kfold1])\n",
    "  results_kfold.head()\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(12,10)) \n",
    "  ax = sns.boxplot(x=\"method\", y=\"mean\", hue=\"method\", data=results_kfold, palette=\"Set1\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0JcOasCLGmR"
   },
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Também será apresentada uma matriz de confusão para o melhor f1_score encontrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "axJkXrymLJaq"
   },
   "outputs": [],
   "source": [
    "def conf_matrix(conf_matrix, list_scores):\n",
    "  index_best = list_scores.index(max(list_scores))\n",
    "  conf_matrix_best = conf_matrix[index_best]\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(15,10)) \n",
    "  sns.set(font_scale=1.4) # for label size\n",
    "  sns.heatmap(conf_matrix_best, annot=True, annot_kws={\"size\": 10}) # font size\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZihAnc1fO409"
   },
   "source": [
    "### Repeated KFold\n",
    "\n",
    "Será realizado um KFold com repetição a fim de comparação com o kfold realizado anteriormente, nesse kfold com repetição não haverá nenhum tipo de tratamento de dados, para assim podermos avaliar se realmente é mais vantajoso realizar uma redução de dimensionalidade nos nossos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "FpZg0fq6O7JP"
   },
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "# evaluate a model with a given number of repeats\n",
    "def evaluate_model(X, y, repeats, model):\n",
    "    # prepare the cross-validation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=repeats, random_state=11)\n",
    "    # create model\n",
    "    model = model\n",
    "    scorer = make_scorer(f1_score, average = 'weighted')\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X, y, scoring=scorer, cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "def repeated_KFold(model, df_all):\n",
    "  # create datasetdata = df_all.values\n",
    "  data = df_all.values\n",
    "  X, y = data[:, :-1], data[:, -1]\n",
    "  # configurations to test\n",
    "  repeats = range(1,6)\n",
    "  results = list()\n",
    "  modelo = model\n",
    "\n",
    "  for r in repeats:\n",
    "    # evaluate using a given number of repeats\n",
    "    scores = evaluate_model(X, y, r, modelo)\n",
    "    # summarize\n",
    "    print('>%d mean=%.4f std=%.3f' % (r, mean(scores), std(scores)))\n",
    "    # store\n",
    "    results.append(scores)\n",
    "  # plot the results\n",
    "  pyplot.boxplot(results, labels=[str(r) for r in repeats], showmeans=True)\n",
    "  pyplot.show()\n",
    "\n",
    "\n",
    "  mean_scores = []\n",
    "  for i in results:\n",
    "    mean_scores.append(mean(i))\n",
    "  best_score = np.amax(mean_scores)\n",
    "\n",
    "  return best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pALo9790cEed"
   },
   "source": [
    "### Avaliação modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "QFliVBylcIsx"
   },
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    y_pred = model.predict(test_features)\n",
    "    acc = accuracy_score(test_labels, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred,labels=[\"cyl\", \"hook\", \"tip\", \"palm\", \"spher\", \"lat\"], average='weighted')\n",
    "    conf_matrix = confusion_matrix(test_labels, y_pred, labels=[\"cyl\", \"hook\", \"tip\", \"palm\", \"spher\", \"lat\"])\n",
    "    print('Model Performance')\n",
    "    print('Accuracy = {:0.5f}%.'.format(acc))\n",
    "    print('F1 = {:0.5f}%.'.format(f1))\n",
    "    \n",
    "    return f1, conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criar X_train, X_test, y_train e y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "def create_X_y_df(df):\n",
    "  # retrieve the array\n",
    "  data = df.values\n",
    "  # split into input and output elements\n",
    "  X, y = data[:, :-1], data[:, -1]\n",
    "  # split into train and test sets\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "  # summarize the shape of the train and test sets\n",
    "  return X_train, X_test, y_train, y_test \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRhSW3nRQZuG"
   },
   "source": [
    "# Algoritmos de Classificação (Modelagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "runYZ1dLK-sG"
   },
   "outputs": [],
   "source": [
    "dict_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9Kl_LMhTBEk"
   },
   "source": [
    "## KNN\n",
    "\n",
    "O algoritmo KNN assume que coisas similares existem em grande proximidade. Em outras palavras, coisas semelhantes estão próximas uma da outra, ele depende desta suposição ser verdadeira o suficiente para que seja útil. O KNN captura a idéia de semelhança (às vezes chamada distância, proximidade ou proximidade) a partir do cálculo da distância entre pontos, a distância em linha reta (também chamada de distância Euclidiana) é uma escolha popular e familiar para calcular a proximidade entre pontos no KNN e, assim, realizar uma tarefa de classificação em diferentes grupos.\n",
    "\n",
    "\n",
    "Abaixo iremos aplicar o algoritmo KNN diretamente nos dados que foram separados em treino e teste para termos um modelo baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsWk4Sxya9uh"
   },
   "source": [
    "### Modelo baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo baseline\n",
      "Model Performance\n",
      "Accuracy = 0.17677%.\n",
      "F1 = 0.07752%.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "X_train, X_test, y_train, y_test = create_X_y_df(df_all)\n",
    "\n",
    "print(\"Modelo baseline\")\n",
    "base_model = KNeighborsClassifier()\n",
    "base_model.fit(X_train, y_train)\n",
    "base, base_conf_matrix = evaluate(base_model, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos um f1_score bastante baixo inicialmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline + PCA \n",
    "\n",
    "\n",
    "Abaixo iremos aplicar o PCA no treino e teste do modelo baseline e realizar seu fit novamente para ver se há alguma melhoria de desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YPfhZFTrZMaJ",
    "outputId": "b22f719e-182c-46a6-a1b9-d51a7cc273ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo com pca\n",
      "Model Performance\n",
      "Accuracy = 0.19865%.\n",
      "F1 = 0.14517%.\n",
      "Melhoria de 87.27547%.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "pca = PCA(.50)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "print(\"Modelo com pca\")\n",
    "base_model = KNeighborsClassifier()\n",
    "base_model.fit(X_train, y_train)\n",
    "base_f1, base_conf_matrix = evaluate(base_model, X_test, y_test)\n",
    "\n",
    "dict_results['knn_baseline'] = base_f1\n",
    "print('Melhoria de {:0.5f}%.'.format( 100 * (base_f1 - base) / base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há uma melhora significativa de desempenho utilizando o PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeDpieM6cTZZ"
   },
   "source": [
    "### Grid Search para achar melhor n de k\n",
    "\n",
    "\n",
    "Para selecionar o K adequado para nossos dados, executamos o algoritmo KNN várias vezes com diferentes valores de K e escolhemos o K que reduz o número de erros que encontramos, mantendo a capacidade do algoritmo de fazer previsões com precisão quando são fornecidos dados que não foram vistos antes.\n",
    "\n",
    "1. À medida que diminuimos o valor de K para 1, nossas previsões se tornam menos estáveis. Basta pensar por um minuto, imagine K=1 e temos um ponto de consulta cercado por vários vermelhos e um verde, mas o verde é o único vizinho mais próximo. Razoavelmente, pensaríamos que o ponto de interrogação é provavelmente vermelho, mas porque K=1, KNN prevê incorretamente que o ponto de interrogação é verde.\n",
    "\n",
    "2. Inversamente, à medida que aumentamos o valor de K, nossas previsões se tornam mais estáveis devido à votação por maioria / média e, portanto, mais propensas a fazer previsões mais precisas (até um certo ponto). Eventualmente, começamos a testemunhar um número crescente de erros. É neste ponto que sabemos que elevamos demais o valor de K.\n",
    "\n",
    "Iremos variar k entre 3 e 31 para o nosso problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQMbVwn5QZVr",
    "outputId": "926026d9-f7b5-4506-e259-e79bcba35f44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n",
      "[CV] END ......................................n_neighbors=3; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=3; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=3; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=3; total time=   0.2s\n",
      "[CV] END ......................................n_neighbors=3; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=3; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=3; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=3; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=3; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=3; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=5; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=5; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=5; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=5; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=5; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=5; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=5; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=5; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=5; total time=   0.2s\n",
      "[CV] END ......................................n_neighbors=5; total time=   0.2s\n",
      "[CV] END ......................................n_neighbors=7; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=7; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=7; total time=   0.2s\n",
      "[CV] END ......................................n_neighbors=7; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=7; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=7; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=7; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=7; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=7; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=7; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=9; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=9; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=9; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=9; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=9; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=9; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=9; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=9; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=9; total time=   0.1s\n",
      "[CV] END ......................................n_neighbors=9; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=11; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=11; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=11; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=11; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=11; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=11; total time=   0.0s\n",
      "[CV] END .....................................n_neighbors=11; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=11; total time=   0.0s\n",
      "[CV] END .....................................n_neighbors=11; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=11; total time=   0.0s\n",
      "[CV] END .....................................n_neighbors=13; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=13; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=13; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=13; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=13; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=13; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=13; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=13; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=13; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=13; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=15; total time=   0.0s\n",
      "[CV] END .....................................n_neighbors=15; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=15; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=15; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=15; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=15; total time=   0.0s\n",
      "[CV] END .....................................n_neighbors=15; total time=   0.0s\n",
      "[CV] END .....................................n_neighbors=15; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=15; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=15; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=17; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=17; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=17; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=17; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=17; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=17; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=17; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=17; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=17; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=17; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=19; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=19; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=19; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=19; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=19; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=19; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=19; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=19; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=19; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=19; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=21; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=21; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=21; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=21; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=21; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=21; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=21; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=21; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=21; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=21; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=23; total time=   0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....................................n_neighbors=23; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=23; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=23; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=23; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=23; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=23; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=23; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=23; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=23; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=25; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=25; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=25; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=25; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=25; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=25; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=25; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=25; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=25; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=25; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=27; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=27; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=27; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=27; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=27; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=27; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=27; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=27; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=27; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=27; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=29; total time=   0.3s\n",
      "[CV] END .....................................n_neighbors=29; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=29; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=29; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=29; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=29; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=29; total time=   0.2s\n",
      "[CV] END .....................................n_neighbors=29; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=29; total time=   0.1s\n",
      "[CV] END .....................................n_neighbors=29; total time=   0.2s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "data = df_all.copy()\n",
    "data = data.values\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "pca = PCA(.50)\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X = scaler.transform(X)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "best_params, best_score = grid_search(knn, {'n_neighbors': np.arange(3, 31, 2)},10, X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y0kvaAE7VBBW",
    "outputId": "9313e732-c203-45b9-f5d0-95f75d943acb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18823898202063633"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check mean score for the top performing value of n_neighbors\n",
    "best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a variação paramétrica conseguimos uma melhoria em relação ao modelo inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhoria de 29.66651%.\n"
     ]
    }
   ],
   "source": [
    "print('Melhoria de {:0.5f}%.'.format( 100 * (best_score - base_f1) / base_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i5jBC918Vfbe",
    "outputId": "de94b82a-2026-4c86-a7ba-4178f7539071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'n_neighbors': 3}\n"
     ]
    }
   ],
   "source": [
    "#check top performing n_neighbors value\n",
    "knn_best = best_params\n",
    "print(\"Melhores parâmetros: {}\".format(knn_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilRwdM0Ssxns"
   },
   "source": [
    "### K-fold com e sem a retirada de outliers\n",
    "\n",
    "A seguir realizaremos uma validação cruzada com o k-fold para avaliar o desempenho do algoritmo, iremos fazer a validação com a base completa e com a base com a retirada de outliers para fins de comparação. Usaremos os parâmetros ótimos encontrados pelo gridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BSrxx3hVyhO",
    "outputId": "0b0a8aa1-48ba-4de2-c4d3-9801e5649615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=11, shuffle=True)\n",
      "--------------------------------------------------------\n",
      "Fold número:  1\n",
      "Tamanho do X_train antes de remover os outliers: (1620, 104)\n",
      "Acurácia antes retirada de outliers:  0.18333333333333332\n",
      "F1-Score antes retirada de outliers:  0.14510993069156758\n",
      "Tamanho do X_train após remover os outliers: (1539, 104)\n",
      "Acurácia após retirada de outliers:  0.18333333333333332\n",
      "F1-Score após retirada de outliers:  0.14510993069156758\n",
      "--------------------------------------------------------\n",
      "Fold número:  2\n",
      "Tamanho do X_train antes de remover os outliers: (1620, 105)\n",
      "Acurácia antes retirada de outliers:  0.16111111111111112\n",
      "F1-Score antes retirada de outliers:  0.13752394558846173\n",
      "Tamanho do X_train após remover os outliers: (1539, 105)\n",
      "Acurácia após retirada de outliers:  0.16111111111111112\n",
      "F1-Score após retirada de outliers:  0.13752394558846173\n",
      "--------------------------------------------------------\n",
      "Fold número:  3\n",
      "Tamanho do X_train antes de remover os outliers: (1620, 104)\n",
      "Acurácia antes retirada de outliers:  0.16666666666666666\n",
      "F1-Score antes retirada de outliers:  0.13945800343872494\n",
      "Tamanho do X_train após remover os outliers: (1539, 104)\n",
      "Acurácia após retirada de outliers:  0.16666666666666666\n",
      "F1-Score após retirada de outliers:  0.13945800343872494\n",
      "--------------------------------------------------------\n",
      "Fold número:  4\n",
      "Tamanho do X_train antes de remover os outliers: (1620, 106)\n",
      "Acurácia antes retirada de outliers:  0.15555555555555556\n",
      "F1-Score antes retirada de outliers:  0.10687808354514784\n",
      "Tamanho do X_train após remover os outliers: (1539, 106)\n",
      "Acurácia após retirada de outliers:  0.15555555555555556\n",
      "F1-Score após retirada de outliers:  0.10687808354514784\n",
      "--------------------------------------------------------\n",
      "Fold número:  5\n",
      "Tamanho do X_train antes de remover os outliers: (1620, 104)\n",
      "Acurácia antes retirada de outliers:  0.17777777777777778\n",
      "F1-Score antes retirada de outliers:  0.1314018449981086\n",
      "Tamanho do X_train após remover os outliers: (1539, 104)\n",
      "Acurácia após retirada de outliers:  0.17777777777777778\n",
      "F1-Score após retirada de outliers:  0.1314018449981086\n",
      "--------------------------------------------------------\n",
      "Fold número:  6\n",
      "Tamanho do X_train antes de remover os outliers: (1620, 106)\n",
      "Acurácia antes retirada de outliers:  0.17777777777777778\n",
      "F1-Score antes retirada de outliers:  0.1373288748288748\n",
      "Tamanho do X_train após remover os outliers: (1539, 106)\n",
      "Acurácia após retirada de outliers:  0.17777777777777778\n",
      "F1-Score após retirada de outliers:  0.1373288748288748\n",
      "--------------------------------------------------------\n",
      "Fold número:  7\n",
      "Tamanho do X_train antes de remover os outliers: (1620, 109)\n",
      "Acurácia antes retirada de outliers:  0.15\n",
      "F1-Score antes retirada de outliers:  0.12356708987533362\n",
      "Tamanho do X_train após remover os outliers: (1539, 109)\n",
      "Acurácia após retirada de outliers:  0.15\n",
      "F1-Score após retirada de outliers:  0.12356708987533362\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f72c6e55c67a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mknn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mknn_best\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnormal_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutlier_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal_conf_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutlier_conf_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal_f1_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutlier_f1_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal_class_repo_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutlier_class_repo_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFolds_outlier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-52f0b68e3c16>\u001b[0m in \u001b[0;36mKFolds_outlier\u001b[0;34m(df_all, model)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Call different fits for either full or truncated SVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'full'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'arpack'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'randomized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit_full\u001b[0;34m(self, X, n_components)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m         \u001b[0;31m# flip eigenvectors' sign to enforce deterministic output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd_flip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/linalg/decomp_svd.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# perform decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,\n\u001b[0m\u001b[1;32m    126\u001b[0m                           full_matrices=full_matrices, overwrite_a=overwrite_a)\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "knn = KNeighborsClassifier(**knn_best)\n",
    "\n",
    "normal_list, outlier_list, normal_conf_matrix, outlier_conf_matrix, normal_f1_list, outlier_f1_list, normal_class_repo_list, outlier_class_repo_list = KFolds_outlier(df_all, knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mc3jGvmCvUcg",
    "outputId": "1074abeb-8e5b-48f2-a54b-f0aa4374be70"
   },
   "outputs": [],
   "source": [
    "print(\"Melhores desempenhos\")\n",
    "print(\"Com outliers:\",max(normal_f1_list))\n",
    "print(\"Sem outliers:\",max(outlier_f1_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Média e Desvio Padrão\")\n",
    "print(\"Sem retirada de outliers\")\n",
    "print(\"Desvio padrão:\",np.std(normal_f1_list))\n",
    "print(\"Média:\", np.mean(normal_f1_list))\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Com retirada de outliers\")\n",
    "print(\"Desvio padrão:\",np.std(outlier_f1_list))\n",
    "print(\"Média:\", np.mean(outlier_f1_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A retirada dos outliers não aparenta tem impacto algum no desempenho do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_results['knn_kfold'] = np.mean(normal_f1_list)\n",
    "dict_results['knn_kfold_outlier'] = np.mean(outlier_f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "id": "UtAGidnWQdpl",
    "outputId": "a7ed7b26-374a-4fe3-ae17-c277fd5ecbe0"
   },
   "outputs": [],
   "source": [
    "boxplot_results(normal_f1_list, outlier_f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "id": "AeN67dHdIBHa",
    "outputId": "4bc8da77-d54a-4778-e550-6fe052cdd4bb"
   },
   "outputs": [],
   "source": [
    "conf_matrix(normal_conf_matrix, normal_f1_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível observar que a matriz de confusão apresenta concentrações nas classes 2, 3 e 5, sendo a 3 a classe mais escolhida pelo modelo, ele parece tender muito para ela, não é um bom classificador. É interessante notar também que ele tende a classificar a classe 4 como 0, mas não classifica absoultamente ninguém para a 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_best = normal_f1_list.index(max(normal_f1_list))\n",
    "class_repo_list_best = normal_class_repo_list[index_best]\n",
    "\n",
    "print(class_repo_list_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6zZ0T0TbyXG"
   },
   "source": [
    "### Repeated K-fold \n",
    "\n",
    "Vamos utilizar o K-Fold repetido para ver o desempenho do modelo sem a realização do tratamento de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "zNfi5ENzb0K3",
    "outputId": "ba6903a1-5a40-4e42-edd7-7ac0fa1e8b18"
   },
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "knn = KNeighborsClassifier(**knn_best)\n",
    "best_score = repeated_KFold(knn, df_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em geral temos um desempenho bastante insatisfatório, o que comprova que é necessaŕia a realização do PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aH2A-4PeogJZ",
    "outputId": "ffa4209c-c1a5-4caf-ac58-53305cddc44e"
   },
   "outputs": [],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gHMdkOmMs-D"
   },
   "outputs": [],
   "source": [
    "dict_results['knn_kfold_repeated'] = best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cog1N0ywUd29"
   },
   "source": [
    "## Decision Tree\n",
    "\n",
    "As árvores de decisão (DTs) são um método de aprendizagem supervisionado não paramétrico utilizado para classificação e regressão. O objetivo é criar um modelo que preveja o valor de uma variável alvo através do aprendizado de regras de decisão simples inferidas a partir das características dos dados. Uma árvore pode ser vista como uma aproximação constante por partes.\n",
    "\n",
    "Algumas vantagens das árvores de decisão são:\n",
    "\n",
    "- Simples de entender e de interpretar. As árvores podem ser visualizadas.\n",
    "\n",
    "- Requer pouca preparação de dados. Outras técnicas muitas vezes requerem normalização dos dados, variáveis fictícias precisam ser criadas e valores em branco precisam ser removidos. Observe, entretanto, que este módulo não suporta valores ausentes.\n",
    "\n",
    "- O custo de usar a árvore (isto é, prever dados) é logarítmico no número de pontos de dados usados para treinar a árvore.\n",
    "\n",
    "- Utiliza um modelo de caixa branca. Se uma determinada situação é observável em um modelo, a explicação da condição é facilmente explicada pela lógica booleana. Em contraste, em um modelo de caixa preta (por exemplo, em uma rede neural artificial), os resultados podem ser mais difíceis de interpretar.\n",
    "\n",
    "- É possível validar um modelo usando testes estatísticos. Isso torna possível contabilizar a confiabilidade do modelo.\n",
    "\n",
    "- Funciona bem mesmo que suas suposições sejam de alguma forma violadas pelo modelo verdadeiro a partir do qual os dados foram gerados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwskRyA8as_e"
   },
   "source": [
    "### Modelo baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "X_train, X_test, y_train, y_test = create_X_y_df(df_all)\n",
    "\n",
    "print(\"Modelo baseline\")\n",
    "base_model = DecisionTreeClassifier()\n",
    "base_model.fit(X_train, y_train)\n",
    "base, base_conf_matrix = evaluate(base_model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "pca = PCA(.50)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "th7CFOlIaJ8c",
    "outputId": "e1bc14da-9800-47bf-ad34-cb220bd4122e"
   },
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "print(\"Modelo baseline com PCA\")\n",
    "base_model = DecisionTreeClassifier()\n",
    "base_model.fit(X_train, y_train)\n",
    "base_f1, base_conf_matrix = evaluate(base_model, X_test, y_test)\n",
    "\n",
    "dict_results['dtree_baseline'] = base_f1\n",
    "\n",
    "print('Melhoria de {:0.5f}%.'.format( 100 * (base_f1 - base) / base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainda não obtivemos um modelo satisfatório, no entanto o f1_score da árvore de decisão já é bem melhor do que o do knn. O PCA melhorou muito pouco os resultados, pode ser que ele não seja ideal ao se lidar com árvores de decisão, iremos constatar isso ao comparar a performance dos kfolds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOkK2_VnUgK-"
   },
   "source": [
    "### Grid Search para achar parâmetros ótimos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sgVF15r5XH3V",
    "outputId": "64fdda64-140c-475e-915e-cd884d3afb8c"
   },
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "data = df_all.copy()\n",
    "data = data.values\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "pca = PCA(.50)\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X = scaler.transform(X)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)\n",
    "\n",
    "max_depth = list(np.arange(3,15))\n",
    "max_depth.append(None)\n",
    "\n",
    "param_grid = { 'criterion':['gini','entropy'],'max_depth': max_depth}\n",
    "dtree = DecisionTreeClassifier()\n",
    "best_params, best_score = grid_search(dtree, param_grid, 10, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TWY6PogHXjEX",
    "outputId": "8bd8fff4-a1c8-471c-d684-285635a49e12"
   },
   "outputs": [],
   "source": [
    "print(best_params)\n",
    "dtree_best = best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OhFIJEo_XkRC",
    "outputId": "336703af-bf75-44ab-c0c8-e530e84b6b52"
   },
   "outputs": [],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FSP_fCkX2qK"
   },
   "source": [
    "### K-Fold com e sem a retirada de outliers\n",
    "\n",
    "\n",
    "A seguir realizaremos uma validação cruzada com o k-fold para avaliar o desempenho do algoritmo, iremos fazer a validação com a base completa e com a base com a retirada de outliers para fins de comparação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yG6AIm5xX7Uu",
    "outputId": "ccad6a78-15d0-4637-b515-b25e9de961f2"
   },
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "dtree = DecisionTreeClassifier(**dtree_best)  \n",
    "normal_list, outlier_list, normal_conf_matrix, outlier_conf_matrix, normal_f1_list, outlier_f1_list, normal_class_repo_list, outlier_class_repo_list = KFolds_outlier(df_all, dtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzstXDrmZQYc",
    "outputId": "45032750-fbbb-4025-d54e-7d0989c42961"
   },
   "outputs": [],
   "source": [
    "print(np.amax(normal_f1_list))\n",
    "print(np.amax(outlier_f1_list))\n",
    "\n",
    "dict_results['dtree_kfold'] = np.mean(normal_f1_list)\n",
    "dict_results['dtree_kfold_outlier'] = np.mean(outlier_f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Melhoria de {:0.5f}%.'.format( 100 * (dict_results['dtree_kfold'] - base_f1) / base_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não há melhoria do modelo ao realizar o fine tuning dos parâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sem retirada de outliers\")\n",
    "print(\"Desvio padrão:\",np.std(normal_f1_list))\n",
    "print(\"Média:\", np.mean(normal_f1_list))\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Com retirada de outliers\")\n",
    "print(\"Desvio padrão:\",np.std(outlier_f1_list))\n",
    "print(\"Média:\", np.mean(outlier_f1_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "rPPfAqxuZxZJ",
    "outputId": "b56a120f-ff93-4bbf-dedf-dd91fba5277e"
   },
   "outputs": [],
   "source": [
    "boxplot_results(normal_f1_list, outlier_f1_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso da árvore de decisão podemos observar que a retirada de outliers não tem o melhor score, mas possui a melhor média e diminui o desvio padrão dos scores, assim sendo preferível à não retirada dos mesmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "d7AZy4cGZ3Gq",
    "outputId": "f90823cc-9667-4efa-9a7e-e31ffc4d5adf"
   },
   "outputs": [],
   "source": [
    "conf_matrix(outlier_conf_matrix, outlier_f1_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A árvore de decisão é definitivamente melhor do que o modelo knn para esse conjunto de dados, no entanto o modelo continua apostando muito na classe 3 e errando com frequencia nas outras, é interessante notar que a 4 (spher) agora está sendo classificada corretamente e com uma precisão alta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_best = outlier_f1_list.index(max(outlier_f1_list))\n",
    "class_repo_list_best = outlier_class_repo_list[index_best]\n",
    "\n",
    "print(class_repo_list_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OO_tr258ZE9X"
   },
   "source": [
    "### Repeated K-Fold\n",
    "\n",
    "Vamos utilizar o K-Fold repetido para ver o desempenho do modelo sem a realização do tratamento de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "eU34o4F1airD",
    "outputId": "1742e27c-45e9-4e43-8a37-70a8509f4329"
   },
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "dtree = DecisionTreeClassifier(**dtree_best)\n",
    "best_score = repeated_KFold(dtree, df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OoVPGN36kbTg",
    "outputId": "3e11a4bf-e8aa-4c9e-cfc5-85c1520868ab"
   },
   "outputs": [],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso da árvore de decisão não houve uma diferença muito brusca entre os desempenhos sem e com o tratamento de dados, mas o sem tratamento de dados tende a ter um f1_score melhor em média."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UCklWdOTMmsC"
   },
   "outputs": [],
   "source": [
    "\n",
    "dict_results['dtree_kfold_repeated'] = best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlRZdBoTgsUS"
   },
   "source": [
    "## Random Forest\n",
    "\n",
    "Florestas aleatórias ou florestas de decisão aleatória são um método de aprendizagem em conjunto para classificação, regressão e outras tarefas que operam através da construção de uma multiplicidade de árvores de decisão em tempo de treinamento e produzindo a classe que é o modo das classes (classificação) ou previsão de média/ média (regressão) das árvores individuais. Florestas de decisão aleatória corrigem o hábito das árvores de decisão de se ajustarem em excesso ao seu conjunto de treinamento. Florestas aleatórias geralmente superam as árvores de decisão, mas sua precisão é menor do que as árvores de decisão de gradiente. Entretanto, as características dos dados podem afetar seu desempenho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmA8aWjXa383"
   },
   "source": [
    "### Modelo baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso da random forest o modelo baseline com PCA possui score significativamente menor do que o sem PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "X_train, X_test, y_train, y_test = create_X_y_df(df_all)\n",
    "\n",
    "\n",
    "base_model = RandomForestClassifier(n_estimators = 10, random_state = 11)\n",
    "base_model.fit(X_train, y_train)\n",
    "base, base_conf_matrix = evaluate(base_model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_results['forest_baseline'] = base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ycAuh31qi132",
    "outputId": "0a1fafa9-5b6d-456d-bbf8-f11f7c4ba017"
   },
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "pca = PCA(.50)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model = RandomForestClassifier(n_estimators = 10, random_state = 11)\n",
    "base_model.fit(X_train, y_train)\n",
    "base_f1, base_conf_matrix = evaluate(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUPciiQRiuxn"
   },
   "source": [
    "### Grid Search para melhorar parâmetros\n",
    "\n",
    "Iremos realizar o grid search duas vezes, para verificar se ele escolhe parâmetros diferentes com a base sem tratamento (já que ela deu melhores resultados inicialmente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcVQa71tj04i"
   },
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "print(\"Grid com X_train e X_test com PCA\")\n",
    "n_estimators = [10, 25]\n",
    "# Number of features to consider at every split\n",
    "crit = ['gini', 'entropy']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = list(np.arange(3, 15))\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'criterion': crit,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "param_grid = { 'criterion':['gini','entropy'],'max_depth': max_depth, 'n_estimators': n_estimators}\n",
    "\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()# Instantiate the grid search model\n",
    "\n",
    "scorer = make_scorer(f1_score, average = 'weighted')\n",
    "\n",
    "grid = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 10, n_jobs = -1, verbose = 2, scoring=scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qIe58Uq6j9_L",
    "outputId": "14220211-54e7-4cac-9f2e-27af9c39ea0c"
   },
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "best_grid = grid.best_estimator_\n",
    "grid_f1, conf_matrix_grid = evaluate(best_grid, X_test, y_test)\n",
    "\n",
    "print('Improvement of {:0.5f}%.'.format( 100 * (grid_f1 - base) / base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-eyGzpgLL5c"
   },
   "outputs": [],
   "source": [
    "forest_pca_best = grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "X_train, X_test, y_train, y_test = create_X_y_df(df_all)\n",
    "\n",
    "print(\"Grid com X_train e X_test sem PCA\")\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()# Instantiate the grid search model\n",
    "\n",
    "scorer = make_scorer(f1_score, average = 'weighted')\n",
    "\n",
    "grid = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 10, n_jobs = -1, verbose = 2, scoring=scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "best_grid = grid.best_estimator_\n",
    "grid_f1, conf_matrix_grid = evaluate(best_grid, X_test, y_test)\n",
    "\n",
    "print('Improvement of {:0.5f}%.'.format( 100 * (grid_f1 - base) / base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_best = grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sem o PCA o grid search conseguiu melhorar o modelo até termos quase 50% em f1_score, no entanto não houve diferença nos parâmetros escolhidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfNq7DbLg5xp"
   },
   "source": [
    "### K-Fold com e sem a retirada de outliers\n",
    "\n",
    "A seguir realizaremos uma validação cruzada com o k-fold para avaliar o desempenho do algoritmo, iremos fazer a validação com a base completa e com a base com a retirada de outliers para fins de comparação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SDMVVAwzg5xq",
    "outputId": "1a1c033b-7d5a-4ee6-efe4-aac4ba75204d"
   },
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "forest = RandomForestClassifier(**forest_best)\n",
    "normal_list, outlier_list, normal_conf_matrix, outlier_conf_matrix, normal_f1_list, outlier_f1_list, normal_class_repo_list, outlier_class_repo_list = KFolds_outlier(df_all, forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF9SYh7sg5xr",
    "outputId": "21361c99-b684-47a6-abd5-0135ca489037"
   },
   "outputs": [],
   "source": [
    "print(np.amax(normal_f1_list))\n",
    "print(np.amax(outlier_f1_list))\n",
    "\n",
    "dict_results['forest_kfold'] = np.mean(normal_f1_list)\n",
    "dict_results['forest_kfold_outlier'] = np.mean(outlier_f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sem retirada de outliers\")\n",
    "print(\"Desvio padrão:\",np.std(normal_f1_list))\n",
    "print(\"Média:\", np.mean(normal_f1_list))\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Com retirada de outliers\")\n",
    "print(\"Desvio padrão:\",np.std(outlier_f1_list))\n",
    "print(\"Média:\", np.mean(outlier_f1_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O kfold com PCA mostra que realmente para a árvore de decisão é melhor seguir sem realizar a redução de dimensionalidade, também podemos observar que nesse caso a retirada de outliers é benéfica para a média de scores, sendo assim preferivel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "OSn0y2Gpg5xs",
    "outputId": "8a91159c-25ba-4031-b6de-1d0d3047d169"
   },
   "outputs": [],
   "source": [
    "boxplot_results(normal_f1_list, outlier_f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "pXeg1ZMqg5xu",
    "outputId": "1ec31c2a-116f-4ad7-f7e6-4573e614de1e"
   },
   "outputs": [],
   "source": [
    "conf_matrix(outlier_conf_matrix, outlier_f1_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na random forest podemos ver que a classe 4 (spher) está quase 100% classificada corretamente, houve também uma melhora significativa nas classificações das classes 3, 2 e 1, no entanto ainda não temos um bom preditor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_best = outlier_f1_list.index(max(outlier_f1_list))\n",
    "class_repo_list_best = outlier_class_repo_list[index_best]\n",
    "\n",
    "print(class_repo_list_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rbUAFbUg5x0"
   },
   "source": [
    "### Repeated K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "# evaluate a model with a given number of repeats\n",
    "def evaluate_model(X, y, repeats, model):\n",
    "    # prepare the cross-validation procedure\n",
    "    scores = []\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=repeats, random_state=11)\n",
    "    # create model\n",
    "    model = model\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        class_repo = classification_report(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred, labels=[\"cyl\", \"hook\", \"tip\", \"palm\", \"spher\", \"lat\"])\n",
    "        f1 = f1_score(y_test, y_pred,labels=[\"cyl\", \"hook\", \"tip\", \"palm\", \"spher\", \"lat\"], average='weighted')\n",
    "        ds = {\"f1\": f1, \"acc\": acc, \"classification_report\": class_repo, \"conf_matrix\": conf_matrix}\n",
    "        scores.append(ds)\n",
    "        \n",
    "    return scores\n",
    "\n",
    "def repeated_KFold(model, df_all):\n",
    "  # create datasetdata = df_all.values\n",
    "  data = df_all.values\n",
    "  X, y = data[:, :-1], data[:, -1]\n",
    "  # configurations to test\n",
    "  repeats = range(1,6)\n",
    "  results = list()\n",
    "  modelo = model\n",
    "\n",
    "  all_scores = []\n",
    "  for r in repeats:\n",
    "    # evaluate using a given number of repeats\n",
    "    scores = evaluate_model(X, y, r, modelo)\n",
    "    f1_list = []\n",
    "    \n",
    "    for i in scores:\n",
    "        f1_list.append(i[\"f1\"])\n",
    "        \n",
    "    # summarize\n",
    "    print('>%d mean=%.4f std=%.3f' % (r, mean(f1_list), std(f1_list)))\n",
    "    # store\n",
    "    results.append(f1_list)\n",
    "    all_scores.append(scores)\n",
    "  # plot the results\n",
    "  pyplot.boxplot(results, labels=[str(r) for r in repeats], showmeans=True)\n",
    "  pyplot.show()\n",
    "\n",
    "\n",
    "  mean_scores = []\n",
    "  for i in results:\n",
    "    mean_scores.append(mean(i))\n",
    "  best_score = np.amax(mean_scores)\n",
    "\n",
    "  return best_score, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "J03fZSfwg5x0",
    "outputId": "54c0bce7-fe08-4753-9a15-f6cfe771070c"
   },
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "forest = RandomForestClassifier(**forest_best)\n",
    "best_score, scores = repeated_KFold(forest, df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification report e matriz de confusão da melhor iteração do repeated kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores[0][8][\"classification_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10)) \n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(scores[0][8][\"conf_matrix\"], annot=True, annot_kws={\"size\": 10}) # font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-HzuiROOHTS"
   },
   "outputs": [],
   "source": [
    "dict_results['forest_kfold_repeated'] = best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui vemos que a classe 4 está quase que perfeitamente prevista enquanto que todas as outras (menos a 3) aumentaram um pouco a acurácia da previsão. Temos o nosso melhor preditor até agora, no entanto ele ainda não é um preditor bom, ou sequer mediano para o problema, pois não acerta pelo menos mais da metade das previsões de cada classe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaiipSg0LL5i"
   },
   "source": [
    "## MLP\n",
    "\n",
    "Um perceptron multicamadas (MLP) é uma classe de rede neural artificial feedforward (ANN). Ele consiste de pelo menos três camadas de nós: uma camada de entrada, uma camada oculta e uma camada de saída, exceto pelos nós de entrada, cada nó é um neurônio que usa uma função de ativação não linear. O MLP utiliza uma técnica de aprendizagem supervisionada chamada backpropagation para treinamento, suas múltiplas camadas e ativação não linear distinguem o MLP de um perceptron linear. Ele pode distinguir dados que não são separáveis linearmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqsOYbjsbHdY"
   },
   "source": [
    "### Modelo baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = create_X_y_df(df_all)\n",
    "\n",
    "print(\"Modelo baseline\")\n",
    "base_model = MLPClassifier()\n",
    "base_model.fit(X_train, y_train)\n",
    "base, base_conf_matrix = evaluate(base_model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo baseline do MLP não chega a ser melhor do que o da random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OgblyYK5H0oH",
    "outputId": "8cca5aff-e259-492f-ef44-5f6f49971d94"
   },
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "pca = PCA(.50)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "print(\"Modelo baseline + PCA\")\n",
    "base_model = MLPClassifier()\n",
    "base_model.fit(X_train, y_train)\n",
    "base_f1, base_conf_matrix = evaluate(base_model, X_test, y_test)\n",
    "\n",
    "dict_results['mlp_baseline'] = base_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MLP aparentemente tem uma performance melhor quando é realizado o PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUeKubVBbYMu"
   },
   "source": [
    "### GridSearch para melhorar parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DsmTH081uJam",
    "outputId": "e9862dcd-9ab3-46c1-b5aa-66ecd4e4394a"
   },
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "data = df_all.copy()\n",
    "data = data.values\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "pca = PCA(.50)\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X = scaler.transform(X)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAgo8USkkgI3",
    "outputId": "91eb30af-2cd1-43aa-d4ee-6b60ae88af8c"
   },
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "#hidden_layer_sizes=100, activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n",
    "\n",
    "hidden_layer_sizes = [100, 120, 150, 200]\n",
    "activation = ['identity', 'logistic', 'tanh', 'relu']\n",
    "solver = ['lbfgs', 'sgd', 'adam']\n",
    "#max_iter = [400]\n",
    "\n",
    "param_grid = {'hidden_layer_sizes': hidden_layer_sizes, 'activation': activation, 'solver': solver}\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "best_params, best_score = grid_search(mlp, param_grid,10, X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qq3qN7sxuNTt",
    "outputId": "34ae4254-b1e3-4d67-fe2d-522fbb5371ea"
   },
   "outputs": [],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1-L52CXY0XBw",
    "outputId": "3fba1e39-daf3-427b-b29d-f790650c2de8"
   },
   "outputs": [],
   "source": [
    "mlp_best = best_params\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P39mMjwwcuO6"
   },
   "source": [
    "### K-Fold com e sem a retirada de outliers\n",
    "\n",
    "A seguir realizaremos uma validação cruzada com o k-fold para avaliar o desempenho do algoritmo, iremos fazer a validação com a base completa e com a base com a retirada de outliers para fins de comparação. Usaremos os parâmetros ótimos encontrados pelo gridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hnCL9QKMcqnb",
    "outputId": "6e61289e-d055-4a95-9317-fa0184d4828c"
   },
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "mlp = MLPClassifier(**mlp_best)\n",
    "normal_list, outlier_list, normal_conf_matrix, outlier_conf_matrix, normal_f1_list, outlier_f1_list, normal_class_repo_list, outlier_class_repo_list = KFolds_outlier(df_all, mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VX0vcCbac9ME",
    "outputId": "caabf059-6cec-41b4-c6b9-db4d0df9388f"
   },
   "outputs": [],
   "source": [
    "print(np.amax(normal_f1_list))\n",
    "print(np.amax(outlier_f1_list))\n",
    "\n",
    "dict_results['mlp_kfold'] = np.mean(normal_f1_list)\n",
    "dict_results['mlp_kfold_outlier'] = np.mean(outlier_f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sem retirada de outliers\")\n",
    "print(\"Desvio padrão:\",np.std(normal_f1_list))\n",
    "print(\"Média:\", np.mean(normal_f1_list))\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Com retirada de outliers\")\n",
    "print(\"Desvio padrão:\",np.std(outlier_f1_list))\n",
    "print(\"Média:\", np.mean(outlier_f1_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A retirada de outliers aparenta prejudicar a média do modelo no caso do mlp, no entanto como esperado ela diminui o seu desvio padrão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "-bNHzuo3c_5A",
    "outputId": "d455a35e-9ce2-4ba8-ccaa-55035823d8e1"
   },
   "outputs": [],
   "source": [
    "boxplot_results(normal_f1_list, outlier_f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "q7EtKPQOdCJu",
    "outputId": "90e9d589-83b2-4561-dbb0-bbdb64eefa57"
   },
   "outputs": [],
   "source": [
    "conf_matrix(normal_conf_matrix, normal_f1_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matriz de confusão do mlp mostra que ele não consegue distinguir bem as classes (exceto a spher), sua performance é bastante parecida com a da árvore de decisão em questão de score, em relação a matriz a árvore de decisão consegue distinguir melhor a classe 3 (palm) do que o mlp. A random forest segue sendo o melhor modelo até então."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_best = normal_f1_list.index(max(normal_f1_list))\n",
    "class_repo_list_best = normal_class_repo_list[index_best]\n",
    "\n",
    "print(class_repo_list_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gd0kTHNbdI3O"
   },
   "source": [
    "### Repeated K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARUYM8HLIv8g"
   },
   "outputs": [],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O kfold repetido apenas confirma o que foi observado anteriormente: para o mlp é necessário realizar o tratamento dos dados com redução de dimensionalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLRFOc39OtMB"
   },
   "outputs": [],
   "source": [
    "dict_results['mlp_kfold_repeated'] = best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNZqeomVsrTh"
   },
   "source": [
    "## Ensemble: Neural Networks\n",
    "\n",
    "\n",
    "### Stacking\n",
    "\n",
    "Stacked Generalization ou simplesmente stacking é um algoritmo de aprendizagem de máquinas de ensemble.\n",
    "\n",
    "Ele envolve a combinação das previsões de vários modelos de aprendizagem de máquina no mesmo conjunto de dados, assim como boosting e bagging.\n",
    "\n",
    "O stacking aborda o questionamento a seguir:\n",
    "\n",
    "\n",
    ">Dados os múltiplos modelos de aprendizagem de máquinas que são hábeis num problema, mas de formas diferentes, como se escolhe qual o modelo a utilizar?\n",
    "\n",
    "\n",
    "A abordagem utilizada é usar outro modelo de aprendizagem de máquina que aprende quando utilizar ou confiar em cada modelo do conjunto.\n",
    "\n",
    "\n",
    "A arquitetura de um modelo de empilhamento envolve dois ou mais modelos de base, frequentemente referidos como modelos de nível 0, e um meta-modelo que combina as previsões dos modelos de base, referidos como modelos de nível 1.\n",
    "\n",
    "> 1. Modelos de nível-0 (Modelos de Base): Modelos treinados com os dados de treino e cujas previsões são compiladas.\n",
    "> 2. Modelo de nível-1 (Meta-Modelo): Modelo que aprende a melhor forma de combinar as previsões dos modelos de base.\n",
    "\n",
    "Iremos realizar um stacking com 3 redes neurais mlp diferentes para observar se o comitê apresenta alguma melhoria em relação a rede mlp original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B633vhBIs2EX"
   },
   "source": [
    "## Ensemble: modelos criados anteriormente\n",
    "\n",
    "\n",
    "O mesmo método de stacking será utilizado para combinar todos os modelos criados anteriormente (com seus melhores parâmetros) e visualizar se há alguma melhoria em relação ao desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZlcz-UXtFIv"
   },
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "data = df_all \n",
    "print(data.shape) \n",
    "data = data.values\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "def get_stacking():\n",
    "  mlp = MLPClassifier(**mlp_best, early_stopping=False, random_state=seed)\n",
    "  forest = RandomForestClassifier(**forest_best)\n",
    "  dtree = DecisionTreeClassifier(**dtree_best)\n",
    "  knn = KNeighborsClassifier(**knn_best)\n",
    "\n",
    "  # Definindo os modelos base do stacking\n",
    "  level0 = list()\n",
    "  level0.append(('mlp-nn', mlp))\n",
    "  level0.append(('knn', knn))\n",
    "  level0.append(('dtree', dtree))\n",
    "  level0.append(('forest', forest))\n",
    "  \n",
    "\t# definindo o modelo meta learner \n",
    "  level1 = LogisticRegression()\n",
    "  # definindo o ensemble\n",
    "  model = StackingClassifier(estimators=level0, final_estimator=level1, cv=10)\n",
    "  return model\n",
    " \n",
    "\n",
    "# função retorna os modelos a serem avaliados\n",
    "def get_models():\n",
    "  mlp = MLPClassifier(**mlp_best, early_stopping=False, random_state=seed)\n",
    "  forest = RandomForestClassifier(**forest_best)\n",
    "  dtree = DecisionTreeClassifier(**dtree_best)\n",
    "  knn = KNeighborsClassifier(**knn_best)\n",
    "\n",
    "  models = dict()\n",
    "  models['mlp-nn'] = mlp\n",
    "  models['knn'] = knn\n",
    "  models['dtree'] = dtree\n",
    "  models['forest'] = forest\n",
    "  models['stacking'] = get_stacking()\n",
    "  return models\n",
    " \n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=seed)\n",
    "    scorer = make_scorer(f1_score, average = 'weighted')\n",
    "    scores = cross_val_score(model, X, y, scoring=scorer, cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores\n",
    "\n",
    "models = get_models()\n",
    "\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble com Boosting\n",
    "\n",
    "Os métodos de boosting funcionam na seguinte forma: construímos uma família de modelos que são agregados para obter um *learner* forte que tenha um melhor desempenho. \n",
    "\n",
    "No entanto, ao contrário do bagging que visa principalmente reduzir a variância, o boosting é uma técnica que consiste em encaixar sequencialmente vários *learners* fracos de uma forma muito adaptativa: cada modelo na sequência é encaixado dando mais importância às observações no conjunto de dados que foram mal tratadas pelos modelos anteriores na sequência. \n",
    "\n",
    "Intuitivamente, cada novo modelo concentra os seus esforços nas observações mais difíceis de encaixar até agora, de modo a obter, no final do processo, um *learner* forte com menor bias (mesmo que se note que o boosting também pode ter o efeito de reduzir a variância). O boosting, tal como o bagging, pode ser utilizado para a regressão, bem como para problemas de classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "seed = 11\n",
    "np.random.seed(seed)\n",
    "\n",
    "def get_models():\n",
    "  mlp = MLPClassifier(**mlp_best, early_stopping=False, random_state=seed)\n",
    "  forest = RandomForestClassifier(**forest_best)\n",
    "  dtree = DecisionTreeClassifier(**dtree_best)\n",
    "  knn = KNeighborsClassifier(**knn_best)\n",
    "  n_trees = [10, 15, 25, 50, 100, 500, 1000]\n",
    "\n",
    "  models = dict()\n",
    "  models['mlp-nn'] = mlp\n",
    "  models['knn'] = knn\n",
    "  models['dtree'] = dtree\n",
    "  models['forest'] = forest\n",
    "\n",
    "  #param_base = {'criterion': 'gini', 'max_depth': None}\n",
    "  forest_base = RandomForestClassifier(**forest_best)\n",
    "\n",
    "  # Variando os modelos\n",
    "  base_estimators = [forest_base] # não é possível realizar a variação dos modelos pois é necessário \n",
    "  # suporte para sample weighting, bem como atributos classes_ e n_classes_ adequados. \n",
    "  for n in n_trees:\n",
    "    for b in base_estimators:\n",
    "      models[str(n)] = AdaBoostClassifier(n_estimators=n, base_estimator = b)\n",
    "  return models\n",
    " \n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=seed)\n",
    "    scorer = make_scorer(f1_score, average = 'weighted')\n",
    "    scores = cross_val_score(model, X, y, scoring=scorer, cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    " \n",
    "\n",
    "models = get_models()\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    " \n",
    "\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "Outra modalidade de boosting que iremos utilizar é o XGBoost, que é basicamente um Gradient Boosting de árvores de decisão, como a random forest (conjunto de árvores de decisão aleatorizadas) foi nosso melhor modelo é esperado que o XGBoost performe pelo menos melhor do que apenas a árvore de decisão sozinha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:53:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "import xgboost as XGB\n",
    "\n",
    "X_train, X_test, y_train, y_test = create_X_y_df(df_all)\n",
    "\n",
    "xgb = XGB.XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=[\"cyl\", \"hook\", \"tip\", \"palm\", \"spher\", \"lat\"])\n",
    "base = f1_score(y_test, y_pred,labels=[\"cyl\", \"hook\", \"tip\", \"palm\", \"spher\", \"lat\"], average='weighted')\n",
    "class_repo = classification_report(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49034274679715967"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_results[\"xgb_baseline\"] = base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "pca = PCA(.50)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=[\"cyl\", \"hook\", \"tip\", \"palm\", \"spher\", \"lat\"])\n",
    "base_pca = f1_score(y_test, y_pred,labels=[\"cyl\", \"hook\", \"tip\", \"palm\", \"spher\", \"lat\"], average='weighted')\n",
    "class_repo = classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch para refinamento de parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = create_X_y_df(df_all)\n",
    "\n",
    "clf = XGB.XGBClassifier()\n",
    "parameters = {\n",
    "     \"eta\"    : [0.05, 0.10] ,\n",
    "     \"min_child_weight\" : [ 1, 3 ],\n",
    "     \"gamma\"            : [ 0.0, 0.1],\n",
    "     \"colsample_bytree\" : [ 0.3, 0.5 ]\n",
    "     }\n",
    "\n",
    "scorer = make_scorer(f1_score, average = 'weighted')\n",
    "grid = GridSearchCV(clf,\n",
    "                    parameters, n_jobs=4,\n",
    "                    scoring=scorer,\n",
    "                    cv=3, verbose=2)\n",
    "\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "print(grid.best_params_)\n",
    "best_grid = grid.best_estimator_\n",
    "grid_f1, conf_matrix_grid = evaluate(best_grid, X_test, y_test)\n",
    "\n",
    "xgb_best = grid.best_params_\n",
    "print('Improvement of {:0.5f}%.'.format( 100 * (grid_f1 - base) / base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando um modelo XGBoost com os parâmetros refinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_test, y_train, y_test = create_X_y_df(df_all)\n",
    "\n",
    "xgb = xgb.XGBClassifier(**xgb_best)\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=[\"cyl\", \"hook\", \"tip\", \"palm\", \"spher\", \"lat\"])\n",
    "f1 = f1_score(y_test, y_pred,labels=[\"cyl\", \"hook\", \"tip\", \"palm\", \"spher\", \"lat\"], average='weighted')\n",
    "class_repo = classification_report(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_results[\"xgb_grid\"] = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10)) \n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 10}) # font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict_results.items():\n",
    "    if key.split('_')[0] == \"knn\":\n",
    "        print(\"{}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict_results.items():\n",
    "    if key.split('_')[0] == \"mlp\":\n",
    "        print(\"{}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict_results.items():\n",
    "    if key.split('_')[0] == \"dtree\":\n",
    "        print(\"{}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict_results.items():\n",
    "    if key.split('_')[0] == \"forest\":\n",
    "        print(\"{}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict_results.items():\n",
    "    if key.split('_')[0] == \"xgb\":\n",
    "        print(\"{}: {}\".format(key,value))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Cópia de Basic_Hand_Gestures_Classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
